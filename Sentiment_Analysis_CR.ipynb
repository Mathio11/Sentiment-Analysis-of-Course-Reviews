{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Model imports:\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ],
      "metadata": {
        "id": "GTOOdNgQfXyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELIAaUFl4wJo"
      },
      "source": [
        "## NAIVE BAYES"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing and Augmentations"
      ],
      "metadata": {
        "id": "rJc65gZ3hi8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Preprocessing and cleaning text\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    words = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join(word for word in words if word not in stop_words)\n",
        "    return text\n",
        "\n",
        "# Synonym-based augmentation\n",
        "def augmented_text(text):\n",
        "    words = text.split()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        syns = wordnet.synsets(word)\n",
        "        if syns:\n",
        "            synonyms = syns[0].lemma_names()\n",
        "            if synonyms:\n",
        "                word = synonyms[0]\n",
        "        if random.uniform(0, 1) > 0.2:\n",
        "            new_words.append(word)\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "\n",
        "data = pd.read_csv('reviews.csv')\n",
        "\n",
        "# Generating cleaned and augmented versions\n",
        "augmented_data = []\n",
        "for index, row in data.iterrows():\n",
        "    cleaned_text = clean_text(row['Review'])\n",
        "    augmented = augmented_text(row['Review'])\n",
        "    augmented_data.append((cleaned_text, augmented, row['Label']))\n",
        "\n",
        "# Converting rating to sentiment classes\n",
        "def define_sentiment(rating):\n",
        "    if isinstance(rating, int) and rating in range(1, 6):\n",
        "        if rating >= 4:\n",
        "            return 'Pos'\n",
        "        elif rating == 3:\n",
        "            return 'Neut'\n",
        "        else:\n",
        "            return 'Neg'\n",
        "    else:\n",
        "        return 'unknown'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_OJBUZLve954"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataframe creation and conversions"
      ],
      "metadata": {
        "id": "oMM8CJ0xh-Cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating DataFrame with processed text and sentiment labels\n",
        "augmented_df = pd.DataFrame(augmented_data, columns=['cleaned_text', 'augmented_text', 'Label'])\n",
        "augmented_df['sentiment'] = augmented_df['Label'].apply(define_sentiment)\n",
        "filtered_data = augmented_df[augmented_df['sentiment'] != 'unknown']\n",
        "\n",
        "# Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "filtered_data['sentiment_class'] = label_encoder.fit_transform(filtered_data['sentiment'])\n",
        "\n",
        "# Vectorization\n",
        "vectorizer_cleaned = CountVectorizer()\n",
        "X_cleaned = vectorizer_cleaned.fit_transform(filtered_data['cleaned_text'])\n",
        "vectorizer_augmented = CountVectorizer()\n",
        "X_augmented = vectorizer_augmented.fit_transform(filtered_data['augmented_text'])\n",
        "\n",
        "\n",
        "# Applying SMOTE to balance class distributions\n",
        "smote_augmented = SMOTE(random_state=42)\n",
        "X_augmented_resampled, y_augmented_resampled = smote_augmented.fit_resample(X_augmented, filtered_data['sentiment_class'])\n",
        "smote_cleaned = SMOTE(random_state=42)\n",
        "X_cleaned_resampled, y_cleaned_resampled = smote_cleaned.fit_resample(X_cleaned, filtered_data['sentiment_class'])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "keyDBWoHhYXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Training/Testing BLOCK"
      ],
      "metadata": {
        "id": "yCciKqY1iEVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train/Testing BLOCK\n",
        "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned_resampled, y_cleaned_resampled, test_size=0.2, random_state=42)\n",
        "X_train_augmented, X_test_augmented, y_train_augmented, y_test_augmented = train_test_split(X_augmented_resampled, y_augmented_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Naive Bayes model\n",
        "def train_evaluate_model_nb(X_train, y_train, X_test, y_test, text_type):\n",
        "    nb_classifier = MultinomialNB()\n",
        "    nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Training\n",
        "    y_pred_train = nb_classifier.predict(X_train)\n",
        "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
        "\n",
        "    # Cross-validation\n",
        "    scores_cv = cross_val_score(nb_classifier, X_train, y_train, cv=10)\n",
        "    mean_accuracy_cv = scores_cv.mean()\n",
        "\n",
        "    # Testing\n",
        "    y_pred_test = nb_classifier.predict(X_test)\n",
        "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "    report_test = classification_report(y_test, y_pred_test, target_names=['Neg', 'Neut', 'Pos'])\n",
        "    conf_matrix_test = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "    # Accuracies\n",
        "    class_labels = label_encoder.classes_\n",
        "    class_accuracies = {}\n",
        "    for label in range(len(class_labels)):\n",
        "        mask = (y_test == label)\n",
        "        class_accuracy = accuracy_score(y_test[mask], y_pred_test[mask])\n",
        "        class_accuracies[class_labels[label]] = class_accuracy\n",
        "\n",
        "    # Printing\n",
        "    print(f\"\\nResults for {text_type} Text using Naive Bayes:\")\n",
        "    print(f'Training Accuracy: {accuracy_train}')\n",
        "    print(f'Mean Cross Validation Accuracy: {mean_accuracy_cv}')\n",
        "    print(f'Test Set Accuracy: {accuracy_test}')\n",
        "    print(\"Class-wise Accuracies for Test Set:\")\n",
        "    for label, acc in class_accuracies.items():\n",
        "        print(f\"{label} Accuracy: {acc}\")\n",
        "    print(\"Classification Report for Test Set:\")\n",
        "    print(report_test)\n",
        "    print(\"Confusion Matrix for Test Set:\")\n",
        "    print(conf_matrix_test)\n",
        "\n",
        "    return nb_classifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "nb_classifier_cleaned = train_evaluate_model_nb(X_train_cleaned, y_train_cleaned, X_test_cleaned, y_test_cleaned, \"Cleaned\")\n",
        "nb_classifier_augmented = train_evaluate_model_nb(X_train_augmented, y_train_augmented, X_test_augmented, y_test_augmented, \"Augmented\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PcvwlyhfhcMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting and Results"
      ],
      "metadata": {
        "id": "uvzLkhnziKjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PLotting\n",
        "# Sentiment distribution\n",
        "def plot_sentiment_distribution(data, title):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    data.value_counts().sort_index().plot(kind='bar', color='blue' if 'Original' in title else ('green' if 'Cleaned' in title else 'orange'))\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Sentiment Class')\n",
        "    plt.ylabel('Number of Sentences')\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.show()\n",
        "\n",
        "# Displaying predicted labels\n",
        "def make_predictions(classifier, X_test, y_test, text_type):\n",
        "    y_pred = classifier.predict(X_test)\n",
        "    predicted_labels = label_encoder.inverse_transform(y_pred)\n",
        "    predictions = pd.DataFrame({'True_Label': label_encoder.inverse_transform(y_test), 'Predicted_Label': predicted_labels})\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    predictions['Predicted_Label'].value_counts().sort_index().plot(kind='bar', color='green' if text_type == \"Cleaned\" else 'orange')\n",
        "    plt.title(f'Distribution of Predicted Sentiment Classes for {text_type} Text')\n",
        "    plt.xlabel('Sentiment Class')\n",
        "    plt.ylabel('Number of Sentences')\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# ROC curve\n",
        "def plot_manual_roc(classifier, X_test, y_test, text_type):\n",
        "    y_score = classifier.predict_proba(X_test)\n",
        "\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    for i in range(len(label_encoder.classes_)):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test, y_score[:, i], pos_label=i)\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    colors = ['blue', 'green', 'orange']\n",
        "    for i, color in zip(range(len(label_encoder.classes_)), colors):\n",
        "        plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve (area = {roc_auc[i]:.2f}) for {label_encoder.classes_[i]}')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC Curve for {text_type} Text')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Plotting/displaying calls\n",
        "plot_sentiment_distribution(data['Label'], 'Distribution of Sentences across Defined Sentiment Classes for Original Dataset')\n",
        "plot_sentiment_distribution(filtered_data['sentiment'], 'Distribution of Sentences across Sentiment Classes for Cleaned Text')\n",
        "plot_sentiment_distribution(augmented_df['sentiment'], 'Distribution of Sentences across Sentiment Classes for Augmented Text')\n",
        "\n",
        "make_predictions(nb_classifier_cleaned, X_test_cleaned, y_test_cleaned, \"Cleaned\")\n",
        "make_predictions(nb_classifier_augmented, X_test_augmented, y_test_augmented, \"Augmented\")\n",
        "\n",
        "plot_manual_roc(nb_classifier_cleaned, X_test_cleaned, y_test_cleaned, \"Cleaned\")\n",
        "plot_manual_roc(nb_classifier_augmented, X_test_augmented, y_test_augmented, \"Augmented\")"
      ],
      "metadata": {
        "id": "hZLpI40zhfbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNl5NrFo4142"
      },
      "source": [
        "## DECISION TREE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing and Augmentations"
      ],
      "metadata": {
        "id": "BlBmznupiSe8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmkkScbb44TV"
      },
      "outputs": [],
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Cleaning text\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    words = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join(word for word in words if word not in stop_words)\n",
        "    return text\n",
        "\n",
        "# Synonym-based augmentation\n",
        "def augmented_text(text):\n",
        "    words = text.split()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        syns = wordnet.synsets(word)\n",
        "        if syns:\n",
        "            synonyms = syns[0].lemma_names()\n",
        "            if synonyms:\n",
        "                word = synonyms[0]\n",
        "        if random.uniform(0, 1) > 0.2:\n",
        "            new_words.append(word)\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "\n",
        "data = pd.read_csv('reviews.csv')\n",
        "\n",
        "# Generating cleaned and augmented versions\n",
        "stop_words = set(stopwords.words('english'))\n",
        "augmented_data = []\n",
        "for index, row in data.iterrows():\n",
        "    cleaned_text = clean_text(row['Review'])\n",
        "    augmented = augmented_text(row['Review'])\n",
        "    augmented_data.append((cleaned_text, augmented, row['Label']))\n",
        "\n",
        "# Converting rating to sentiment classes\n",
        "def define_sentiment(rating):\n",
        "    if isinstance(rating, int) and rating in range(1, 6):\n",
        "        if rating >= 4:\n",
        "            return 'Pos'\n",
        "        elif rating == 3:\n",
        "            return 'Neut'\n",
        "        else:\n",
        "            return 'Neg'\n",
        "    else:\n",
        "        return 'unknown'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataframe creation and conversions"
      ],
      "metadata": {
        "id": "z6UGp5qNii_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Creating DataFrame with processed text and sentiment labels\n",
        "augmented_df = pd.DataFrame(augmented_data, columns=['cleaned_text', 'augmented_text', 'Label'])\n",
        "augmented_df['sentiment'] = augmented_df['Label'].apply(define_sentiment)\n",
        "filtered_data = augmented_df[augmented_df['sentiment'] != 'unknown']\n",
        "\n",
        "# Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "filtered_data['sentiment_class'] = label_encoder.fit_transform(filtered_data['sentiment'])\n",
        "\n",
        "# Vectorization\n",
        "vectorizer_cleaned = CountVectorizer()\n",
        "X_cleaned = vectorizer_cleaned.fit_transform(filtered_data['cleaned_text'])\n",
        "vectorizer_augmented = CountVectorizer()\n",
        "X_augmented = vectorizer_augmented.fit_transform(filtered_data['augmented_text'])\n",
        "\n",
        "# Applying SMOTE to balance class distributions\n",
        "smote_augmented = SMOTE(random_state=42)\n",
        "X_augmented_resampled, y_augmented_resampled = smote_augmented.fit_resample(X_augmented, filtered_data['sentiment_class'])\n",
        "smote_cleaned = SMOTE(random_state=42)\n",
        "X_cleaned_resampled, y_cleaned_resampled = smote_cleaned.fit_resample(X_cleaned, filtered_data['sentiment_class'])\n",
        "\n"
      ],
      "metadata": {
        "id": "8YSF6RRQiX6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Training/Testing BLOCK"
      ],
      "metadata": {
        "id": "W6Fu3oo7ilGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train/Testing BLOCK\n",
        "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned_resampled, y_cleaned_resampled, test_size=0.2, random_state=42)\n",
        "X_train_augmented, X_test_augmented, y_train_augmented, y_test_augmented = train_test_split(X_augmented_resampled, y_augmented_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Decision Tree model\n",
        "def train_evaluate_model_dt(X_train, y_train, X_test, y_test, text_type):\n",
        "    dt_classifier = DecisionTreeClassifier()\n",
        "    dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Training\n",
        "    y_pred_train = dt_classifier.predict(X_train)\n",
        "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
        "\n",
        "    # Cross-validation\n",
        "    scores_cv = cross_val_score(dt_classifier, X_train, y_train, cv=10)\n",
        "    mean_accuracy_cv = scores_cv.mean()\n",
        "\n",
        "    # Testing\n",
        "    y_pred_test = dt_classifier.predict(X_test)\n",
        "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "    report_test = classification_report(y_test, y_pred_test, target_names=['Neg', 'Neut', 'Pos'])\n",
        "    conf_matrix_test = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "    # Accuracies\n",
        "    class_labels = label_encoder.classes_\n",
        "    class_accuracies = {}\n",
        "    for label in range(len(class_labels)):\n",
        "        mask = (y_test == label)\n",
        "        class_accuracy = accuracy_score(y_test[mask], y_pred_test[mask])\n",
        "        class_accuracies[class_labels[label]] = class_accuracy\n",
        "\n",
        "    # Printing\n",
        "    print(f\"\\nResults for {text_type} Text using Decision Tree:\")\n",
        "    print(f'Training Accuracy: {accuracy_train}')\n",
        "    print(f'Mean Cross Validation Accuracy: {mean_accuracy_cv}')\n",
        "    print(f'Test Set Accuracy: {accuracy_test}')\n",
        "    print(\"Class-wise Accuracies for Test Set:\")\n",
        "    for label, acc in class_accuracies.items():\n",
        "        print(f\"{label} Accuracy: {acc}\")\n",
        "    print(\"Classification Report for Test Set:\")\n",
        "    print(report_test)\n",
        "    print(\"Confusion Matrix for Test Set:\")\n",
        "    print(conf_matrix_test)\n",
        "\n",
        "    return dt_classifier\n",
        "\n",
        "# Training Decision Tree models\n",
        "dt_classifier_cleaned = train_evaluate_model_dt(X_train_cleaned, y_train_cleaned, X_test_cleaned, y_test_cleaned, \"Cleaned\")\n",
        "dt_classifier_augmented = train_evaluate_model_dt(X_train_augmented, y_train_augmented, X_test_augmented, y_test_augmented, \"Augmented\")\n"
      ],
      "metadata": {
        "id": "FBj0GT2Iiacf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting and Results"
      ],
      "metadata": {
        "id": "cd761wljimZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plotting\n",
        "# Sentiment distribution\n",
        "def plot_sentiment_distribution(data, title):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    data.value_counts().sort_index().plot(kind='bar', color='blue' if 'Original' in title else ('green' if 'Cleaned' in title else 'orange'))\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Sentiment Class')\n",
        "    plt.ylabel('Number of Sentences')\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.show()\n",
        "\n",
        "# Displaying predicted labels\n",
        "def make_predictions(classifier, X_test, y_test, text_type):\n",
        "    y_pred = classifier.predict(X_test)\n",
        "    predicted_labels = label_encoder.inverse_transform(y_pred)\n",
        "    predictions = pd.DataFrame({'True_Label': label_encoder.inverse_transform(y_test), 'Predicted_Label': predicted_labels})\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    predictions['Predicted_Label'].value_counts().sort_index().plot(kind='bar', color='green' if text_type == \"Cleaned\" else 'orange')\n",
        "    plt.title(f'Distribution of Predicted Sentiment Classes for {text_type} Text')\n",
        "    plt.xlabel('Sentiment Class')\n",
        "    plt.ylabel('Number of Sentences')\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.show()\n",
        "\n",
        "# ROC curve\n",
        "def plot_manual_roc(classifier, X_test, y_test, text_type):\n",
        "    y_score = classifier.predict_proba(X_test)\n",
        "\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    for i in range(len(label_encoder.classes_)):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test, y_score[:, i], pos_label=i)\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    colors = ['blue', 'green', 'orange']\n",
        "    for i, color in zip(range(len(label_encoder.classes_)), colors):\n",
        "        plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve (area = {roc_auc[i]:.2f}) for {label_encoder.classes_[i]}')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC Curve for {text_type} Text')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "# Plotting/displaying calls\n",
        "plot_sentiment_distribution(data['Label'], 'Distribution of Sentences across Defined Sentiment Classes for Original Dataset')\n",
        "plot_sentiment_distribution(filtered_data['sentiment'], 'Distribution of Sentences across Sentiment Classes for Cleaned Text')\n",
        "plot_sentiment_distribution(augmented_df['sentiment'], 'Distribution of Sentences across Sentiment Classes for Augmented Text')\n",
        "\n",
        "make_predictions(dt_classifier_cleaned, X_test_cleaned, y_test_cleaned, \"Cleaned\")\n",
        "make_predictions(dt_classifier_augmented, X_test_augmented, y_test_augmented, \"Augmented\")\n",
        "\n",
        "plot_manual_roc(dt_classifier_cleaned, X_test_cleaned, y_test_cleaned, \"Cleaned\")\n",
        "plot_manual_roc(dt_classifier_augmented, X_test_augmented, y_test_augmented, \"Augmented\")"
      ],
      "metadata": {
        "id": "-Kk7RChBic9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6oUnfRd2wvA"
      },
      "source": [
        "##RANDOM FOREST"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing and Augmentations"
      ],
      "metadata": {
        "id": "KPgLlrcDi5wE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEsSZKxS-Pmi"
      },
      "outputs": [],
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Cleaning text\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    words = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join(word for word in words if word not in stop_words)\n",
        "    return text\n",
        "\n",
        "# Synonym-based augmentation\n",
        "def augmented_text(text):\n",
        "    words = text.split()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        syns = wordnet.synsets(word)\n",
        "        if syns:\n",
        "            synonyms = syns[0].lemma_names()\n",
        "            if synonyms:\n",
        "                word = synonyms[0]\n",
        "        if random.uniform(0, 1) > 0.2:\n",
        "            new_words.append(word)\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('reviews.csv')\n",
        "\n",
        "# Generating cleaned and augmented versions\n",
        "stop_words = set(stopwords.words('english'))\n",
        "augmented_data = []\n",
        "for index, row in data.iterrows():\n",
        "    cleaned_text = clean_text(row['Review'])\n",
        "    augmented = augmented_text(row['Review'])\n",
        "    augmented_data.append((cleaned_text, augmented, row['Label']))\n",
        "\n",
        "# Converting rating to sentiment classes\n",
        "def define_sentiment(rating):\n",
        "    if isinstance(rating, int) and rating in range(1, 6):\n",
        "        if rating >= 4:\n",
        "            return 'Pos'\n",
        "        elif rating == 3:\n",
        "            return 'Neut'\n",
        "        else:\n",
        "            return 'Neg'\n",
        "    else:\n",
        "        return 'unknown'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataframe creation and conversions"
      ],
      "metadata": {
        "id": "mxnckMeUi8R9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Creating DataFrame with processed text and sentiment labels\n",
        "augmented_df = pd.DataFrame(augmented_data, columns=['cleaned_text', 'augmented_text', 'Label'])\n",
        "augmented_df['sentiment'] = augmented_df['Label'].apply(define_sentiment)\n",
        "filtered_data = augmented_df[augmented_df['sentiment'] != 'unknown']\n",
        "\n",
        "# Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "filtered_data['sentiment_class'] = label_encoder.fit_transform(filtered_data['sentiment'])\n",
        "\n",
        "# Vectorization\n",
        "vectorizer_cleaned = CountVectorizer()\n",
        "X_cleaned = vectorizer_cleaned.fit_transform(filtered_data['cleaned_text'])\n",
        "vectorizer_augmented = CountVectorizer()\n",
        "X_augmented = vectorizer_augmented.fit_transform(filtered_data['augmented_text'])\n",
        "\n",
        "# Applying SMOTE to balance class distributions\n",
        "smote_augmented = SMOTE(random_state=42)\n",
        "X_augmented_resampled, y_augmented_resampled = smote_augmented.fit_resample(X_augmented, filtered_data['sentiment_class'])\n",
        "smote_cleaned = SMOTE(random_state=42)\n",
        "X_cleaned_resampled, y_cleaned_resampled = smote_cleaned.fit_resample(X_cleaned, filtered_data['sentiment_class'])\n",
        "\n"
      ],
      "metadata": {
        "id": "CTDK9i2PixFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Training/Testing BLOCK"
      ],
      "metadata": {
        "id": "D9m5UhDJi-D3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/Testing BLOCK\n",
        "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned_resampled, y_cleaned_resampled, test_size=0.2, random_state=42)\n",
        "X_train_augmented, X_test_augmented, y_train_augmented, y_test_augmented = train_test_split(X_augmented_resampled, y_augmented_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest model\n",
        "def train_evaluate_model_rf(X_train, y_train, X_test, y_test, text_type):\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Training\n",
        "    y_pred_train = rf_classifier.predict(X_train)\n",
        "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
        "\n",
        "    # Cross-validation\n",
        "    scores_cv = cross_val_score(rf_classifier, X_train, y_train, cv=10)\n",
        "    mean_accuracy_cv = scores_cv.mean()\n",
        "\n",
        "    # Testing\n",
        "    y_pred_test = rf_classifier.predict(X_test)\n",
        "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "    report_test = classification_report(y_test, y_pred_test, target_names=['Neg', 'Neut', 'Pos'])\n",
        "    conf_matrix_test = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "    # Accuracies\n",
        "    class_labels = label_encoder.classes_\n",
        "    class_accuracies = {}\n",
        "    for label in range(len(class_labels)):\n",
        "        mask = (y_test == label)\n",
        "        class_accuracy = accuracy_score(y_test[mask], y_pred_test[mask])\n",
        "        class_accuracies[class_labels[label]] = class_accuracy\n",
        "\n",
        "    # Printing\n",
        "    print(f\"\\nResults for {text_type} Text using Random Forest:\")\n",
        "    print(f'Training Accuracy: {accuracy_train}')\n",
        "    print(f'Mean Cross Validation Accuracy: {mean_accuracy_cv}')\n",
        "    print(f'Test Set Accuracy: {accuracy_test}')\n",
        "    print(\"Class-wise Accuracies for Test Set:\")\n",
        "    for label, acc in class_accuracies.items():\n",
        "        print(f\"{label} Accuracy: {acc}\")\n",
        "    print(\"Classification Report for Test Set:\")\n",
        "    print(report_test)\n",
        "    print(\"Confusion Matrix for Test Set:\")\n",
        "    print(conf_matrix_test)\n",
        "\n",
        "    return rf_classifier\n",
        "\n",
        "# Training Random Forest models\n",
        "rf_classifier_cleaned = train_evaluate_model_rf(X_train_cleaned, y_train_cleaned, X_test_cleaned, y_test_cleaned, \"Cleaned\")\n",
        "rf_classifier_augmented = train_evaluate_model_rf(X_train_augmented, y_train_augmented, X_test_augmented, y_test_augmented, \"Augmented\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "piAEHYIgizrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting and Results"
      ],
      "metadata": {
        "id": "bkw28kqFjBVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "# Sentiment distribution\n",
        "def plot_sentiment_distribution(data, title):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    data.value_counts().sort_index().plot(kind='bar', color='blue' if 'Original' in title else ('green' if 'Cleaned' in title else 'orange'))\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Sentiment Class')\n",
        "    plt.ylabel('Number of Sentences')\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.show()\n",
        "\n",
        "# Displaying predicted labels\n",
        "def make_predictions(classifier, X_test, y_test, text_type):\n",
        "    y_pred = classifier.predict(X_test)\n",
        "    predicted_labels = label_encoder.inverse_transform(y_pred)\n",
        "    predictions = pd.DataFrame({'True_Label': label_encoder.inverse_transform(y_test), 'Predicted_Label': predicted_labels})\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    predictions['Predicted_Label'].value_counts().sort_index().plot(kind='bar', color='green' if text_type == \"Cleaned\" else 'orange')\n",
        "    plt.title(f'Distribution of Predicted Sentiment Classes for {text_type} Text')\n",
        "    plt.xlabel('Sentiment Class')\n",
        "    plt.ylabel('Number of Sentences')\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.show()\n",
        "\n",
        "# ROC curve\n",
        "def plot_manual_roc(classifier, X_test, y_test, text_type):\n",
        "    y_score = classifier.predict_proba(X_test)\n",
        "\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    for i in range(len(label_encoder.classes_)):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test, y_score[:, i], pos_label=i)\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    colors = ['blue', 'green', 'orange']\n",
        "    for i, color in zip(range(len(label_encoder.classes_)), colors):\n",
        "        plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve (area = {roc_auc[i]:.2f}) for {label_encoder.classes_[i]}')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC Curve for {text_type} Text')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "# Plotting/displaying calls\n",
        "plot_sentiment_distribution(data['Label'], 'Distribution of Sentences across Defined Sentiment Classes for Original Dataset')\n",
        "plot_sentiment_distribution(filtered_data['sentiment'], 'Distribution of Sentences across Sentiment Classes for Cleaned Text')\n",
        "plot_sentiment_distribution(augmented_df['sentiment'], 'Distribution of Sentences across Sentiment Classes for Augmented Text')\n",
        "\n",
        "make_predictions(rf_classifier_cleaned, X_test_cleaned, y_test_cleaned, \"Cleaned\")\n",
        "make_predictions(rf_classifier_augmented, X_test_augmented, y_test_augmented, \"Augmented\")\n",
        "\n",
        "plot_manual_roc(rf_classifier_cleaned, X_test_cleaned, y_test_cleaned, \"Cleaned\")\n",
        "plot_manual_roc(rf_classifier_augmented, X_test_augmented, y_test_augmented, \"Augmented\")"
      ],
      "metadata": {
        "id": "Uh7pXpMUi4Lu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}